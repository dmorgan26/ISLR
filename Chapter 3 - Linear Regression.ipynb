{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): attempt to use zero-length variable name\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): attempt to use zero-length variable name\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "---\n",
    "title: \"Chapter 3 - Linear Regression\"\n",
    "author: \"Dan\"\n",
    "date: \"26 January 2018\"\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---\n",
    "\n",
    "```{r setup, include=FALSE}\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "```\n",
    "```{r}\n",
    "library(MASS)\n",
    "data=Boston\n",
    "names(Boston)\n",
    "\n",
    "## We will seek to predict medv (Median Value of houses in that suburb) using 13 predictors. ?Boston for details \n",
    "```\n",
    "\n",
    "```{r}\n",
    "lm.fit <- lm(medv ~ lstat, data = Boston)\n",
    "summary(lm.fit)\n",
    "```\n",
    "```{r}\n",
    "names(lm.fit)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "confint(lm.fit) # to find 95% confidence intervals, level = 0.95 by default\n",
    "```\n",
    "\n",
    "```{r}\n",
    "predict(lm.fit, data.frame(lstat=c(5,10,15)), interval =\"confidence\") # predicting the value of medv using the lm.fit model. Output data frame where lstat = 5, 10, 15\n",
    "```\n",
    "```{r}\n",
    "predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = \"prediction\") # Prediction intervals include the irreducible error so are wider than confidence intervals\n",
    "```\n",
    "```{r}\n",
    "#Plotting the linear regression line\n",
    "attach(Boston)\n",
    "plot(lstat, medv)\n",
    "abline(lm.fit, col=\"red\")\n",
    "\n",
    "plot(lstat, medv)\n",
    "abline(lm.fit, lwd = 3) # line width\n",
    "\n",
    "plot(lstat, medv, pch=20) # plot character, changes shape\n",
    "plot(lstat, medv, pch=\"*\") # can be any character\n",
    "plot (1:20, 1:20, pch = 1:20) # shows all characters\n",
    "\n",
    "## Lets look at some plots from the lm() function ##\n",
    "## can do plot(lm.fit) to cycle through some relevant plots\n",
    "\n",
    "plot(lm.fit)\n",
    "\n",
    "# Better to look at them all at once #\n",
    "\n",
    "par(mfrow=c(2,2)) # graphics parameters, show 2x2\n",
    "plot(lm.fit)\n",
    "\n",
    "# We can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against fitted values\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "plot(predict(lm.fit), residuals(lm.fit))\n",
    "plot(predict(lm.fit), rstudent(lm.fit))\n",
    "\n",
    "## On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n",
    "\n",
    "plot(hatvalues(lm.fit))\n",
    "which.max(hatvalues(lm.fit))\n",
    "\n",
    "## Multiple Regression ##\n",
    "\n",
    "lm.fit <- lm(medv ~ lstat + age, data = Boston)\n",
    "summary(lm.fit)\n",
    "\n",
    "## To regress against all reminaing variables, we can use a dot to mean all ##\n",
    "\n",
    "lm.fit <- lm(medv ~ ., data = Boston)\n",
    "summary(lm.fit)\n",
    "\n",
    "## We can access individual components of a summary object by name (?lm.summary to see what is available)\n",
    "\n",
    "# car package has the vif() function \n",
    "\n",
    "library(car)\n",
    "vif(lm.fit) # Max value is 9, moderate collinearity but can be ignored < 10\n",
    "\n",
    "## Running lm with all but one syntax, age had high p-value ##\n",
    "\n",
    "lm.fit <- lm(medv ~ .-age, data = Boston)\n",
    "summary(lm.fit)\n",
    "\n",
    "## Interaction term syntax ##\n",
    "# lstat:black includes an interaction term (lstat x black)\n",
    "# lstat*black includes lstat, black and the interaction\n",
    "\n",
    "summary(lm(medv ~ lstat*age, data = Boston))\n",
    "\n",
    "# Non-linear transformations #\n",
    "# Using an X squared transformation needs to be wrapped in the function I() because the hat has a special function in R #\n",
    "\n",
    "lm.fit2 <- lm(medv ~ lstat + I(lstat^2))\n",
    "summary(lm.fit2)\n",
    "\n",
    "## small p-value associated with the quadratic term suggests that it leads to an improved model. We can use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit\n",
    "\n",
    "lm.fit <- lm(medv ~ lstat, data = Boston)\n",
    "anova(lm.fit, lm.fit2) # Anova performs a hypothesis test comparing the two models. H0 = the two models fit the data equally well. F-stat of 135 and p-value near 0 provides clear evidence that model 2 containing the polynomial is superior. \n",
    "\n",
    "par(mfrow=c(2,2))\n",
    "plot(lm.fit5)\n",
    "\n",
    "# Now residuals vs fitted has no discernible pattern\n",
    "# To take higher polynomials we can use the poly() function within lm()\n",
    "\n",
    "lm.fit5 <- lm(medv ~ poly(lstat,5))\n",
    "summary(lm.fit5) # low p-values for all, model fit keeps improving up to the 5th order polynomial. Adjusted Rsquared = 0.679\n",
    "summary(lm.fit) # adjusted Rsquared = 0.543\n",
    "\n",
    "## Log transformation\n",
    "\n",
    "summary(lm(medv ~ log(rm), data = Boston))\n",
    "\n",
    "## Qualitative Predictors ##\n",
    "\n",
    "library(ISLR)\n",
    "data(Carseats)\n",
    "\n",
    "carseats <- Carseats\n",
    "\n",
    "attach(carseats)\n",
    "\n",
    "## Attempting to predict sales in 400 locations based on a numbe of predictors.\n",
    "\n",
    "?Carseats\n",
    "\n",
    "# Variable ShelveLoc measures shelving location in the store as Bad, Good or Medium\n",
    "\n",
    "levels(ShelveLoc)\n",
    "\n",
    "# Given a qualitative variable such as ShelveLoc, R generates dummy variables automatically. Below we fit a simple multiple regression model that includes some interaction terms\n",
    "\n",
    "lm.fit <- lm(Sales ~ .+Income:Advertising+Price:Age, data = carseats)\n",
    "summary(lm.fit) # Coefficients for ShelveGood and ShelveMedium suggest tha effects here are strong\n",
    "\n",
    "mean(carseats$Sales) \n",
    "1.01/7.49  # RSE of 1.01 on mean sales of 7.49 = 13.5%\n",
    "\n",
    "# Call contrasts() function on the variable to see the coding that R uses\n",
    "\n",
    "contrasts(ShelveLoc)\n",
    "\n",
    "## Applied Exercises ##\n",
    "\n",
    "data(Auto)\n",
    "attach(Auto)\n",
    "# 8a\n",
    "\n",
    "lm.fit <- lm(mpg ~ horsepower)\n",
    "summary(lm.fit)\n",
    "\n",
    "\n",
    "\n",
    "## There is a relationship between the predictor and the response\n",
    "## Relationship is pretty strong, Rsquared = 0.605\n",
    "\n",
    "mean(mpg)\n",
    "\n",
    "# %age error = RSE/mean of response = 4.906/23.446 = 20.9%\n",
    "# relationship is negative\n",
    "\n",
    "predict(lm.fit, data.frame(horsepower = c(98)), interval = \"prediction\")\n",
    "predict(lm.fit, data.frame(horsepower = c(98)), interval = \"confidence\")\n",
    "\n",
    "# predicted mpg = 24.467\n",
    "# prediction interval (14.809,34.124)\n",
    "# 95% confidence interval (23.97,24.96)\n",
    "\n",
    "plot(mpg ~ horsepower)\n",
    "abline(lm.fit, col=\"red\", lwd = 1.5)\n",
    "\n",
    "plot(lm.fit)\n",
    "  \n",
    "# Residuals plot suggests highly non-linear relationship\n",
    "\n",
    "## 9\n",
    "data(Auto)\n",
    "\n",
    "plot(Auto)\n",
    "\n",
    "library(dplyr)\n",
    "names(Auto)\n",
    "\n",
    "Auto_no_name <- Auto[,1:8]\n",
    "cormat <- cor(Auto_no_name)\n",
    "\n",
    "summary(lm(mpg ~ .-name, data=Auto))\n",
    "\n",
    "# There is a relationship between the predictors and the response - F test p-value <0.05\n",
    "\n",
    "# displacement, weight, year, origin\n",
    "\n",
    "?Auto\n",
    "table(year)\n",
    "\n",
    "# suggests that for every year older the model is, the mpg increases by 0.75\n",
    "\n",
    "summary(lm(mpg ~ .-name, data = Auto))\n",
    "\n",
    "plot(lm.fit)\n",
    "\n",
    "# Residual plot has a U-shape so relationship is likely non-linear\n",
    "\n",
    "#data points 323, 327 are outliers\n",
    "#data point 14 has very high leverage, car has high horsepower\n",
    "\n",
    "Auto[14,]\n",
    "\n",
    "lm.fit <- lm(mpg ~ year + origin + displacement*weight, data = Auto)\n",
    "summary(lm.fit)\n",
    "\n",
    "# adding displacement and weight interaction took R2 from 0.81 to 0.85 and interaction is statistically significant\n",
    "\n",
    "lm.fit <- lm(mpg ~ year + displacement*weight+log(year), data = Auto)\n",
    "summary(lm.fit)\n",
    "plot(lm.fit)\n",
    "\n",
    "Auto2 <- Auto[-14,]\n",
    "\n",
    "\n",
    "lm.fit <- lm(mpg ~ year + displacement*weight+log(year), data = Auto2)\n",
    "summary(lm.fit)\n",
    "plot(lm.fit)\n",
    "\n",
    "#log year is statistically significant\n",
    "\n",
    "lm.fit <- lm(mpg ~ poly(displacement,3)+weight+year+origin, data = Auto2)\n",
    "summary(lm.fit)\n",
    "\n",
    "#Q10\n",
    "\n",
    "data(\"Carseats\")\n",
    "attach(Carseats)\n",
    "\n",
    "lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)\n",
    "\n",
    "summary(lm.fit)\n",
    "?Carseats\n",
    "\n",
    "# Price - sales drop by 54 (sales are in thousands) for every dollar increase in price\n",
    "\n",
    "# UrbanYes - sales decrese by 21 for urban locations\n",
    "\n",
    "#USYes - sales increase by 1201 when a store is in the US\n",
    "\n",
    "# Y = 13.04 - 0.05 * Price - 0.02*UrbanYes + 1.2 * USYes\n",
    "\n",
    "# Reject the null hypothesis for Urban\n",
    "\n",
    "lm.fit <- lm(Sales ~ Price + US)\n",
    "mean(Sales)\n",
    "summary(lm.fit)\n",
    "\n",
    "#Model A has a RSE of 2.472 so %age error = 2.472/7.5 = 33% and R^2 = 0.2335\n",
    "\n",
    "#Model B has a RSE of 2.469 so %age error = 33% and R^2 = 0.2345\n",
    "\n",
    "# Model B is a slightly better fit\n",
    "\n",
    "confint(lm.fit, level = 0.95)\n",
    "\n",
    "# Confidence intervals\n",
    "#Price = (-0.065, -0.044)\n",
    "#USYes = (0.692, 1.708)\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "plot(lm.fit)\n",
    "library(car)\n",
    "plot(hatvalues(lm.fit))\n",
    "which.max(hatvalues(lm.fit))\n",
    "\n",
    "# Leverage plot shows one value with leverage of over 0.04. Average leverage for the data set = (p+1/n) = 3/400 = 0.0075 so one data point of 0.04 leverage could be a problem.\n",
    "\n",
    "## Q11\n",
    "\n",
    "set.seed(1)\n",
    "x <- rnorm(100)\n",
    "y <- 2*x+rnorm(100)\n",
    "  \n",
    "\n",
    "lm.fit <- lm(y ~ x+0) # +0 means without an intercept\n",
    "summary(lm.fit)\n",
    "\n",
    "# Coefficient estimate = 1.9939\n",
    "# RSE = 0.9586\n",
    "# t-stat = 18.73\n",
    "# p-value = <2e-16\n",
    "\n",
    "# Suggests that the coefficient x is highly significant and that a one unit increase in x increases the response y on average by 1.9939. We reject the null that Beta = 0\n",
    "\n",
    "lm.fit <- lm(x~y+0)\n",
    "summary (lm.fit)\n",
    "\n",
    "# Coefficient estimate = 0.39111\n",
    "# RSE = 0.4246\n",
    "# t-stat = 18.73\n",
    "# p-value = <2e-16\n",
    "\n",
    "# Suggests that the coefficient estimate of y is highly significant, when y increases, espected increase in x of 0.3911. We reject the null that Beta = 0\n",
    "\n",
    "# Q13\n",
    "\n",
    "set.seed(1)\n",
    "x <- rnorm(100, mean = 0, sd = 1)\n",
    "eps <- rnorm(100, mean = 0, sd = 0.25)\n",
    "Y <- -1 + 0.5*(x) + eps*(x)\n",
    "length(Y)\n",
    " \n",
    "#length = 100\n",
    "# Beta0 = -1, Beta1 = 0.5\n",
    "\n",
    "plot(x, Y) # relationship looks reasonably linear\n",
    "\n",
    "lm.fit <- lm(Y ~ x)\n",
    "summary(lm.fit)\n",
    "\n",
    "# The model shows that x is statistically significant, Beta values are close at -1 and 0.548\n",
    "\n",
    "abline(lm.fit, lty=1) # least squares regression line\n",
    "abline(-1, 0.5, col=\"red\", lty=2) # population regression line\n",
    "\n",
    "\n",
    "lm.fit2 <- lm(Y ~ x+I(x^2))\n",
    "summary(lm.fit2)\n",
    "abline(lm.fit2, col=\"green\", lty=3)\n",
    "\n",
    "legend(\"topleft\", legend = c(\"Least Squares\",\"Population\",\"Polynomial\"), col=c(\"black\",\"red\",\"green\"), lty=1:3)\n",
    "\n",
    "anova(lm.fit, lm.fit2) # ANOVA gives high p-value so model fit is not improved\n",
    "\n",
    "### Again with less noise in the data, lowever the variance of x\n",
    "\n",
    "set.seed(1)\n",
    "x <- rnorm(100, mean = 0, sd = 1)\n",
    "eps <- rnorm(100, mean = 0, sd = 0.05)\n",
    "Y <- -1 + 0.5*(x) + eps*(x)\n",
    "length(Y)\n",
    " \n",
    "#length = 100\n",
    "# Beta0 = -1, Beta1 = 0.5\n",
    "\n",
    "plot(x, Y) # relationship looks reasonably linear\n",
    "\n",
    "lm.fit <- lm(Y ~ x)\n",
    "summary(lm.fit)\n",
    "\n",
    "# The model shows that x is statistically significant, Beta values are close at -1 and 0.548\n",
    "\n",
    "abline(lm.fit, lty=1) # least squares regression line\n",
    "abline(-1, 0.5, col=\"red\", lty=2) # population regression line\n",
    "\n",
    "\n",
    "lm.fit2 <- lm(Y ~ x+I(x^2))\n",
    "summary(lm.fit2)\n",
    "abline(lm.fit2, col=\"green\", lty=3)\n",
    "\n",
    "legend(\"topleft\", legend = c(\"Least Squares\",\"Population\",\"Polynomial\"), col=c(\"black\",\"red\",\"green\"), lty=1:3)\n",
    "\n",
    "anova(lm.fit, lm.fit2)\n",
    "\n",
    "# Here the polynomial is better\n",
    "# Here the polynomial is better too\n",
    "\n",
    "#Confints\n",
    "\n",
    "confint(lm.fit)\n",
    "\n",
    "#Noisy (0.3848,0.5842)\n",
    "#Original (0.4478, 0.5526)\n",
    "#Less Noisy (0.4974, 0.5189)\n",
    "\n",
    "# More noise = wider confidence intervals, makes sense with higher variance of error terms\n",
    "\n",
    "# Q14\n",
    "\n",
    "set.seed(1)\n",
    "x1 <- runif(100)\n",
    "x2 <- 0.5*x1+rnorm(100)/10\n",
    "y <- 2+2*x1+0.3*x2+rnorm(100)\n",
    "\n",
    "\n",
    "## Y = 2 + 2(X1) + 0.3(X2) + e\n",
    "## B0 = 2, B1 = 2, B2 = 0.3\n",
    "\n",
    "x1 <- c(x1, 0.1)\n",
    "x2 <- c(x2, 0.8)\n",
    "y <- c(y,6)\n",
    "\n",
    "plot(x1,x2) # weak positive correlation\n",
    "\n",
    "lm.fit <- lm(y ~ x1 + x2)\n",
    "summary(lm.fit)\n",
    "plot(lm.fit)\n",
    "\n",
    "# x2 has very large standard error and is not significantly different from zero\n",
    "\n",
    "# B0 = 2.0113, B1 = 2.2993, B2 = -0.2352\n",
    "\n",
    "# We can reject the null that x1 = 0 but not that x2 = 0\n",
    "\n",
    "lm.fit2 <- lm(y~x1)\n",
    "summary(lm.fit2)\n",
    "plot(lm.fit2)\n",
    "\n",
    "\n",
    "# We can reject the null that x1 is zero\n",
    "\n",
    "lm.fit3 <- lm(y~x2)\n",
    "summary(lm.fit3)\n",
    "plot(lm.fit3)\n",
    "\n",
    "# We can reject the null that x2 is zero\n",
    "\n",
    "# These results do not contradict each other. Without the presence of other predictors, both B1 and B2 are statistically significant. In the presence of other predictors, B2 is no longer statistically significant\n",
    "\n",
    "#In the both model, the additions make them both statistically significant, data point 101 is high leverage, > 0.3 when average leverage = (p+1)/n = 4/101 = 0.039\n",
    "\n",
    "#Second model, x1 is still significant, data point 101 is an outlier or high leverage\n",
    "\n",
    "#Third model, x2 is still significant, data point 101 is high leverage\n",
    "\n",
    "plot(x1, y)\n",
    "plot(x2, y)\n",
    "\n",
    "#Q15\n",
    "\n",
    "data(Boston)\n",
    "\n",
    "names(Boston)\n",
    "\n",
    "lm1 <- lm(crim ~ zn)\n",
    "summary(lm1) # zn in statistically significant\n",
    "\n",
    "summary(lm(crim ~ indus)) # sig\n",
    "summary(lm(crim ~ chas))\n",
    "summary(lm(crim ~ nox)) # sig\n",
    "summary(lm(crim ~ rm)) # sig\n",
    "summary(lm(crim ~ age)) # sig\n",
    "summary(lm(crim ~ dis)) # sig\n",
    "summary(lm(crim ~ rad)) # sig\n",
    "summary(lm(crim ~ tax)) # sig\n",
    "summary(lm(crim ~ ptratio)) #all rest are sig\n",
    "summary(lm(crim ~ black))\n",
    "summary(lm(crim ~ lstat))\n",
    "summary(lm(crim ~ medv))\n",
    "\n",
    "lm.fit <- lm(crim ~. , data = Boston)\n",
    "summary(lm.fit)\n",
    "\n",
    "# Reject the null for everything with a significance code\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
