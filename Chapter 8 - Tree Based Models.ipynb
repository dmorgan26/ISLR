{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Chapter 8 - Tree Based Models\"\n",
    "author: \"Dan\"\n",
    "date: \"19 February 2018\"\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---\n",
    "\n",
    "```{r setup, include=FALSE}\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "```\n",
    "```{r}\n",
    "# The tree library is used to construct classification and regression trees\n",
    "\n",
    "library(tree)\n",
    "\n",
    "# We first use classification trees to analyse the Carseats data. In these data, Sales is a continuous variable, and so we begin by recording it as a binary variable. We use ifelse() function to create a variable called High, which takes on the value Yes if the Sales variable exceeds 8, and takes on a value of No otherwise.\n",
    "\n",
    "library(ISLR)\n",
    "attach(Carseats)\n",
    "\n",
    "High <- ifelse(Sales <= 8, \"No\", \"Yes\")\n",
    "\n",
    "# Finally, we merge High with the rest of the Carseats data\n",
    "\n",
    "Carseats <- data.frame(Carseats, High)\n",
    "\n",
    "# We now used the tree() function to fit a classification tree in order to predict High using all of the variables except Sales. The syntax of tree() is quite similar to that of lm()\n",
    "\n",
    "tree.carseats <- tree(High~.-Sales, data=Carseats)\n",
    "\n",
    "# The summary() function lists the variables that are used in the internal nodes in the tree, the number of terminal nodes, and the (training) error rate. \n",
    "\n",
    "summary(tree.carseats)\n",
    "\n",
    "# We see that the training error rate is 9%. For classification trees, the deviance reported in the output os summary() is given on page 339. A small deviance indicates a tree that provides a good fit to the (training) data. The residual mean deviance reported is simply the deviance divided by n-|T0|, which in this case is 400-27=373. \n",
    "\n",
    "# One of the most attractive properties of trees is that they can be graphically displayed. We use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty=0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category.\n",
    "\n",
    "plot(tree.carseats)\n",
    "text(tree.carseats, pretty=0)\n",
    "\n",
    "# The most important indicator of Sales appears to be shelving location, since the first branch differentiates Good locations from Bad and Medium locations.\n",
    "\n",
    "# If we just type the name of the tree object, R prints output corresponding to each branch of the tree. R displays the split criterion (e.g. Price<92.5), the number of observations in that branch, the deviance, the overall prediction for the branch (Yes or No), and the fraction of observations in that branch that take on values of Yes and No. Branches that lead to terminal nodes are indicated using asterisks\n",
    "\n",
    "tree.carseats\n",
    "\n",
    "# In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. The predict() function can be used for this purpose. In the case of a classification tree, the argument type=\"class\" instructs R to return the actual class prediction. This approach leads to correct predictions for around 71.5 % of the locations in the test data set.\n",
    "\n",
    "set.seed(2)\n",
    "train <- sample(1:nrow(Carseats), 200)\n",
    "\n",
    "Carseats.test <- Carseats[-train,]\n",
    "High.test <- High[-train]\n",
    "\n",
    "tree.carseats <- tree(High~.-Sales, data=Carseats, subset=train)\n",
    "\n",
    "tree.pred <- predict(tree.carseats, newdata = Carseats.test, type=\"class\")\n",
    "\n",
    "table(tree.pred, High.test)\n",
    "\n",
    "(86+57)/200 # = 71.5% correct predictions\n",
    "\n",
    "# Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration.  We use the argument FUN=prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to alpha in (8.4)).\n",
    "\n",
    "set.seed(3)\n",
    "cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)\n",
    "names(cv.carseats)\n",
    "cv.carseats\n",
    "\n",
    "# Note that, despite the name, dev corresponds to the cross-validation error rate in this instance. The tree with 9 terminal nodes results in the lowest cross-validation error rate, with 50 cross-validation errors. We plot the error rate as a function of both size and k.\n",
    "\n",
    "par(mfrow=c(1,2))\n",
    "\n",
    "plot(cv.carseats$size, cv.carseats$dev, type=\"b\")\n",
    "plot(cv.carseats$k, cv.carseats$dev, type=\"b\")\n",
    "\n",
    "# We now apply the prune.misclass() function in order to prune the tree to obtain the nine-node tree\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "\n",
    "pruned.carseats <- prune.misclass(tree.carseats, best=9)\n",
    "plot(pruned.carseats)\n",
    "text(pruned.carseats, pretty=0)\n",
    "\n",
    "# How well does this pruned tree perform on the test data? Once again we apply the predict() function\n",
    "\n",
    "tree.pred <- predict(pruned.carseats, newdata = Carseats.test, type=\"class\")\n",
    "table(tree.pred, High.test)\n",
    "\n",
    "(94+60)/200 # = 77% correctly classified. Not only has the pruning process produced a more interpretable tree, it has also improved the classification accuracy\n",
    "\n",
    "# If we increase the value of best, we obtain a larger pruned tree with lower classification accuracy.\n",
    "\n",
    "##########\n",
    "# Fitting Regression Trees\n",
    "##########\n",
    "\n",
    "# Here we fit a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data.\n",
    "\n",
    "library(MASS)\n",
    "set.seed(1)\n",
    "train <- sample(1:nrow(Boston), nrow(Boston)/2)\n",
    "tree.boston <- tree(medv~., data=Boston, subset=train)\n",
    "summary(tree.boston)\n",
    "\n",
    "# Notice that the output of summary() indicates that only three of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree.\n",
    "\n",
    "plot(tree.boston)\n",
    "text(tree.boston, pretty=0)\n",
    "\n",
    "# The variable lstat measures the percentage of individuals with lower socioeconomic status. The tree indicates that lower values of lstat correspond to more expensive houses. The tree predicts a median house price of $46, 400 for larger homes in suburbs in which residents have high socioeconomic status (rm>=7.437 and lstat<9.715)\n",
    "\n",
    "# Now we use the cv.tree() function to see whether pruning the tree can improve performance\n",
    "\n",
    "cv.boston <- cv.tree(tree.boston) # Don't have to specify FUN=prune.misclass here because its the deviance that we want in regression problems\n",
    "\n",
    "cv.boston\n",
    "plot(cv.boston$size, cv.boston$dev, type=\"b\")\n",
    "\n",
    "# Here the most complex tree is chosen by cross validation. However, if we wish to prune the tree, we could do so as follows, using the prune.tree() function\n",
    "\n",
    "pruned.boston <- prune.tree(tree.boston, best=5)\n",
    "plot(pruned.boston)\n",
    "text(pruned.boston, pretty=0)\n",
    "\n",
    "# In keeping with the cross validation results, we use the unpruned tree to make predictions on the test set.\n",
    "\n",
    "yhat <- predict(tree.boston, newdata = Boston[-train,])\n",
    "boston.test <- Boston[-train,\"medv\"]\n",
    "plot(yhat, boston.test)\n",
    "abline(0,1)\n",
    "mean((yhat-boston.test)^2) # = 25.05\n",
    "\n",
    "# In other words, the test MSE associated with the regression tree is 25.05. The square root of the MSE is therefore around 5.005 indicating that this model leads to test predictions that are within around $5005 of the true median home value for the suburb.\n",
    "\n",
    "## Bagging and Random Forests ##\n",
    "\n",
    "# Here we apply bagging and random forests to the Boston data, using the randomForest package. Recall that bagging is simply a special case of a random forest with m=p. Therefore, randomForest() function can be used to perform both random forests and bagging.\n",
    "\n",
    "## Bagging ##\n",
    "\n",
    "library(randomForest)\n",
    "set.seed(1)\n",
    "bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, importance=TRUE)\n",
    "\n",
    "bag.boston\n",
    "\n",
    "# The argument mtry=13 indicates that all 13 predictors should be considered for each split of the tree - in other words, that bagging should be done. How well does this bagged model perform on the test set?\n",
    "\n",
    "yhat.bag <- predict(bag.boston, newdata = Boston[-train,])\n",
    "plot(yhat.bag, boston.test)\n",
    "abline(0,1)\n",
    "mean((yhat.bag-boston.test)^2) # MSE = 13.47\n",
    "sqrt(13.47) # = 3.67 so the bagged prediction is out by an average of $3,670\n",
    "\n",
    "# The test MSE associated with the bagged regression tree is almost half that obtained using an optimally-pruned single tree. We could change the number of trees grown by randomForest() using the ntree argument:\n",
    "\n",
    "bag.boston <- randomForest(medv~.,data=Boston, subset=train, mtry=13, ntree=25)\n",
    "\n",
    "yhat.bag <- predict(bag.boston, newdata = Boston[-train,])\n",
    "mean((yhat.bag-boston.test)^2) # = 13.43\n",
    "\n",
    "# Growing a random forest proceeds in exactly the same way, except that we use a smaller value for the mtry argument. By default, randomForest uses p/3 variables when building a forest of regression trees, and sqrt(p) variables when building a random forest of classification trees. Here we use mtry=6\n",
    "\n",
    "set.seed(1)\n",
    "rf.boston <- randomForest(medv~.,data=Boston, subset=train, mtry=6, importance=TRUE)\n",
    "\n",
    "yhat.rf <- predict(rf.boston, newdata = Boston[-train,])\n",
    "mean((yhat.rf-boston.test)^2) # = 11.48 which is an improvement on the bagging case\n",
    "\n",
    "# Using the importance() function, we can view the importance of each variable\n",
    "\n",
    "importance(rf.boston)\n",
    "\n",
    "# Two measures of variable importance are reported. The former is based upon the mean decrease of accuracy of predictions on the out of bag samples when a given variable is left out of the model. The latter is a measure of the tota decrease in node impurity that results from splits over that variable, arranged over all trees (this was plotted in Fig 8.9). In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can be produced using the varImpPlot() function.\n",
    "\n",
    "varImpPlot(rf.boston)\n",
    "\n",
    "# The results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variables.\n",
    "\n",
    "## Boosting ##\n",
    "\n",
    "# Here we use the gbm package, and within it the gbm() function , to fit boosted regression trees to the Boston data set. We run gbm() with the option distribution=\"gaussian\" since this is a regression problem; if it were a binary classification problem, we would use distribution=\"bernoulli\". The argument n.trees=5000 indicates that we want 5000 trees, and the option interaction.depth=4 limits the depth of each tree\n",
    "\n",
    "library(gbm)\n",
    "set.seed(1)\n",
    "boost.boston <- gbm(medv~., data=Boston[train,], distribution=\"gaussian\", n.trees=5000, interaction.depth = 4)\n",
    "\n",
    "# The summary() function produces a relative influence plot and also outputs the relative influence statistics.\n",
    "\n",
    "summary(boost.boston)\n",
    "\n",
    "# We see that lstat and rm are by far the most important variables. We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. In this case, as we might expect, median house prices are increaseing with rm and decreasing with lstat\n",
    "\n",
    "par(mfrow=c(1,2))\n",
    "plot(boost.boston, i=\"rm\")\n",
    "plot(boost.boston, i=\"lstat\")\n",
    "\n",
    "# We now use the boosted model to precict medv on the test set:\n",
    "\n",
    "yhat.boost <- predict(boost.boston, newdata=Boston[-train,], n.trees = 5000)\n",
    "mean((yhat.boost-boston.test)^2) # = 11.84 which is similar to the test MSE for random forests and superior to that for bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter lambda in (8.10). The default value is 0.001, but this is easily modified. Here we take lambda=0.2\n",
    "\n",
    "boost.boston <- gbm(medv~., data=Boston[train,], distribution = \"gaussian\", n.tree=5000, interaction.depth = 4, shrinkage = 0.2, verbose = F ) # verbose shows stuff in the console\n",
    "\n",
    "yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)\n",
    "\n",
    "mean((yhat.boost-boston.test)^2) # = 11.43\n",
    "\n",
    "# In this case, using lambda=0.2 leads to a slightly improved MSE than lambda=0.001\n",
    "\n",
    "#######\n",
    "# Exercises\n",
    "#######\n",
    "\n",
    "#3)\n",
    "\n",
    "p <- seq(0,1,0.01)\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "\n",
    "plot(NA, NA, type=\"n\", xlim=c(0,1), ylim=c(0,1), xlab=\"p\", ylab=\"Value\")\n",
    "\n",
    "gini <- p*(1-p) * 2 # \n",
    "entropy <- -1*(p*log(p) + (1-p)*log(1-p))\n",
    "class.error <- 1-pmax(p,1-p)\n",
    "\n",
    "matplot(p, cbind(gini, entropy, class.error), col=c(\"red\",\"green\",\"blue\"))\n",
    "\n",
    "\n",
    "\n",
    "legend(\"topright\", legend=c(\"Gini\",\"Entropy\",\"Classification\"), col=c(\"red\",\"blue\",\"green\"), lty=1)\n",
    "\n",
    "# 7) \n",
    "mtry13.errors <- mtry8.errors <- mtry5.errors <- mtry3.errors <- mtry2.errors <- rep(NA,500)\n",
    "\n",
    "\n",
    "for (i in 1:500) {\n",
    "boston.rf <- randomForest(medv~., data=Boston[train,], mtry=2, ntree=i)\n",
    "boston.rf.pred <- predict(boston.rf, newdata=Boston[-train,])\n",
    "mtry2.errors[i] <- mean((boston.rf.pred-boston.test)^2)\n",
    "}\n",
    "\n",
    "matplot(cbind(mtry13.errors, mtry8.errors, mtry5.errors, mtry3.errors, mtry2.errors), type=\"l\", col=c(\"green\",\"blue\",\"brown\",\"red\",\"black\"), lwd=3)\n",
    "\n",
    "legend(\"topright\", legend=c(\"mtry=13\",\"mtry=8\",\"mtry=5\",\"mtry=3\",\"mtry=2\"), col=c(\"green\",\"blue\",\"brown\",\"red\",\"black\"), lty=1, cex=1)\n",
    "\n",
    "##################\n",
    "# The way they did it uses MSE from the randomforest function, inputting all of x, y, xtest and ytest\n",
    "\n",
    "set.seed(1)\n",
    "train <- sample(1:nrow(Boston), nrow(Boston) / 2)\n",
    "Boston.train <- Boston[train, -14]\n",
    "Boston.test <- Boston[-train, -14]\n",
    "Y.train <- Boston[train, 14]\n",
    "Y.test <- Boston[-train, 14]\n",
    "rf.boston1 <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = ncol(Boston) - 1, ntree = 500)\n",
    "rf.boston2 <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = (ncol(Boston) - 1) / 2, ntree = 500)\n",
    "rf.boston3 <- randomForest(Boston.train, y = Y.train, xtest = Boston.test, ytest = Y.test, mtry = sqrt(ncol(Boston) - 1), ntree = 500)\n",
    "plot(1:500, rf.boston1$test$mse, col = \"green\", type = \"l\", xlab = \"Number of Trees\", ylab = \"Test MSE\", ylim = c(10, 19))\n",
    "lines(1:500, rf.boston2$test$mse, col = \"red\", type = \"l\")\n",
    "lines(1:500, rf.boston3$test$mse, col = \"blue\", type = \"l\")\n",
    "legend(\"topright\", c(\"m = p\", \"m = p/2\", \"m = sqrt(p)\"), col = c(\"green\", \"red\", \"blue\"), cex = 1, lty = 1)\n",
    "\n",
    "#8a)\n",
    "\n",
    "data(\"Carseats\")\n",
    "set.seed(1)\n",
    "\n",
    "trainid <- sample(1:nrow(Carseats), nrow(Carseats)/2)\n",
    "\n",
    "carseats.train <- Carseats[trainid,]\n",
    "carseats.test <- Carseats[-trainid,]\n",
    "\n",
    "tree.carseats <- tree(Sales~., data=carseats.train)\n",
    "pred.tree <- predict(tree.carseats, newdata = carseats.test)\n",
    "mean((pred.tree-carseats.test$Sales)^2)\n",
    "# = 4.149\n",
    "\n",
    "cv.carseats <- cv.tree(tree.carseats)\n",
    "which.min(cv.carseats$dev) # size of 8 terminal nodes has smallest CV error\n",
    "\n",
    "plot(cv.carseats, type=\"b\")\n",
    "\n",
    "prune.carseats <- prune.tree(tree.carseats, best = 8)\n",
    "prune.preds <- predict(prune.carseats, newdata = carseats.test)\n",
    "mean((prune.preds-carseats.test$Sales)^2) # MSE increased to 5.09\n",
    "\n",
    "carseats.train <- Carseats[trainid,]\n",
    "carseats.test <- Carseats[-trainid,]\n",
    "\n",
    "y.train <- Carseats$Sales[trainid]\n",
    "y.test <- Carseats$Sales[-trainid]\n",
    "\n",
    "\n",
    "bag.carseats <- randomForest(Sales~., data=carseats.train,  mtry=ncol(carseats.train)-1, ntree=500, importance=T)\n",
    "\n",
    "yhat.bag <- predict(bag.carseats, newdata = carseats.test)\n",
    "\n",
    "mean((yhat.bag-y.test)^2) # MSE = 2.64\n",
    "\n",
    "importance(bag.carseats)\n",
    "\n",
    "# Price and ShelveLoc are the two most important variables\n",
    "\n",
    "rf.carseats <- randomForest(Sales~., data=carseats.train, mtry=3)\n",
    "\n",
    "pred.rf <- predict(rf.carseats, newdata = carseats.test)\n",
    "\n",
    "mean((pred.rf-y.test)^2) # = 3.28\n",
    "importance(rf.carseats) # Still price and shelveloc. Decreasing m has much lowered the error rate!\n",
    "\n",
    "#9a)\n",
    "\n",
    "data(OJ)\n",
    "attach(OJ)\n",
    "\n",
    "set.seed(1)\n",
    "trainid <- sample(1:nrow(OJ), 800, replace=FALSE)\n",
    "\n",
    "train <- OJ[trainid,]\n",
    "test <- OJ[-trainid,]\n",
    "\n",
    "#b)\n",
    "\n",
    "oj.tree <- tree(Purchase~., data=train)\n",
    "summary(oj.tree)\n",
    "\n",
    "# Tree uses 4 variables, training error rate = 16.5%\n",
    "# Tree has 8 terminal nodes\n",
    "\n",
    "#c)\n",
    "oj.tree\n",
    "\n",
    "plot(oj.tree)\n",
    "text(oj.tree, pretty=0)\n",
    "\n",
    "# interpreting 11) The split criterion is PriceDiff > 0.195, there are 101 observations in that node with deviance of 139.2 and the prediction for 45.5% of those observations that are in the terminal node is CH\n",
    "\n",
    "# LoyalCH is the most important criterion. The customers with brand loyalty to Cyprus Hill >= 0.51 all those Cyprus Hill. Among the less loyal customers, all of those with loyalty < 0.26 chose Minute Maid. Customers with CH loyalty between 0.26 and 0.5 CH unless the price difference was < 0.195 and there was no special offer on CH.\n",
    "\n",
    " # Customers with very low loyalty to CH or customers with low brand loyalty and very small price difference and no special offer chose to buy MM\n",
    "\n",
    "#e)\n",
    "\n",
    "test.y <- test$Purchase\n",
    "\n",
    "pred.tree <- predict(oj.tree, newdata = test, type = \"class\")\n",
    "\n",
    "table(pred.tree, test.y)\n",
    "\n",
    "dim(test)\n",
    "\n",
    "(12+49)/270 # = test error rate of 22.6%\n",
    "\n",
    "#f)\n",
    "oj.cv <- cv.tree(oj.tree, FUN=prune.misclass)\n",
    "\n",
    "# Here dev means the cross validation error, the value is the same for all tree sizes down to two so we'll use that for ease of interpretation. Plotting shows up something different though, always good practice to plot\n",
    "\n",
    "#g)\n",
    "plot(oj.cv$size, oj.cv$dev, type=\"b\")\n",
    "\n",
    "#h) Optimal tree size = 5\n",
    "\n",
    "#i)\n",
    "oj.prune <- prune.tree(oj.tree, best=2)\n",
    "\n",
    "plot(oj.prune)\n",
    "text(oj.prune, pretty=0)\n",
    "\n",
    "summary(oj.tree) # = 16.5% training error rate\n",
    "summary(oj.prune)# = 18.25% training error rate\n",
    "\n",
    "prune.pred <- predict(oj.prune, test, type=\"class\")\n",
    "table(prune.pred, test$Purchase)\n",
    "\n",
    "1-(119+81)/270 # = 26% test error rate vs 22.6% test error rate, so the pruning decreased the model performance.\n",
    "\n",
    "#10a)\n",
    "library(ISLR)\n",
    "data(Hitters)\n",
    "attach(Hitters)\n",
    "\n",
    "summary(Hitters)\n",
    "\n",
    "hitters2 <- na.omit(Hitters)\n",
    "hitters2$Salary <- log(hitters2$Salary) # log transformation solves histogram with lots of right skew, can bring it towards normality\n",
    "\n",
    "#b)\n",
    "dim(hitters2) \n",
    "\n",
    "trainid <- 1:200\n",
    "\n",
    "train <- hitters2[trainid,]\n",
    "test <- hitters2[-trainid,]\n",
    "\n",
    "#c)\n",
    "library(gbm)\n",
    "\n",
    "set.seed(1)\n",
    "powers <- seq(-10,-0.2,by=0.1)\n",
    "lambdas <- 10^powers\n",
    "\n",
    "train.err <- rep(NA,99)\n",
    "y.train <- hitters2$Salary[trainid]\n",
    "\n",
    "for(i in 1:99) {\n",
    "  \n",
    "  boost.hitters <- gbm(Salary~., data=train, n.trees = 1000, shrinkage = lambdas[i], distribution = \"gaussian\")\n",
    "  boost.pred <- predict(boost.hitters, newdata = train, n.trees = 1000)\n",
    "  train.err[i] <- mean((boost.pred-y.train)^2)\n",
    "  \n",
    "}\n",
    "\n",
    "plot(lambdas, train.err, type=\"b\")\n",
    "\n",
    "\n",
    "#d)\n",
    "\n",
    "powers <- seq(-3,-1,by=0.1)\n",
    "lambdas <- 10^powers\n",
    "\n",
    "test.err <- rep(NA,length(lambdas))\n",
    "y.test <- hitters2$Salary[-trainid]\n",
    "\n",
    "for(i in 1:length(lambdas)) {\n",
    "  \n",
    "  boost.hitters <- gbm(Salary~., data=train, n.trees = 1000, shrinkage = lambdas[i], distribution = \"gaussian\")\n",
    "  boost.pred <- predict(boost.hitters, newdata = test, n.trees = 1000)\n",
    "  test.err[i] <- mean((boost.pred-y.test)^2)\n",
    "  \n",
    "}\n",
    "\n",
    "plot(lambdas, test.err, type=\"b\")\n",
    "\n",
    "which.min(test.err)\n",
    "lambdas[15] # 0.025 is best value for lambda\n",
    "\n",
    "#e)\n",
    "\n",
    "boost.hitters <- gbm(Salary~., data=train, n.trees = 1000, shrinkage = 0.025, distribution = \"gaussian\")\n",
    "boost.pred <- predict(boost.hitters, newdata = test, n.trees = 1000)\n",
    "mse.boost <- mean((boost.pred-y.test)^2) # MSE = 0.264\n",
    "mse.boost\n",
    "\n",
    "## Comparing the MSE for boosting vs MSE for linear regression lasso and KNN models ##\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "modelmat.train <- as.data.frame(model.matrix(Salary~., data=train))\n",
    "modelmat.test <- as.data.frame(model.matrix(Salary~., data=test))\n",
    "\n",
    "train <- hitters2[trainid,]\n",
    "test <- hitters2[-trainid,]\n",
    "test.y <- hitters2$Salary[-trainid]\n",
    "\n",
    "hitters.lm <- lm(Salary~., data=train)\n",
    "hitters.pred <- predict(hitters.lm, newdata=test)\n",
    "mse.lm <- mean((hitters.pred-test.y)^2) # MSE = 0.49\n",
    "\n",
    "## Ridge Regression ##\n",
    "\n",
    "modelmat.train <- model.matrix(Salary~., data=train)[,-1]\n",
    "\n",
    "library(glmnet)\n",
    "\n",
    "## docs say to use a grid for possible lambda values ##\n",
    "\n",
    "grid <- 10^seq(10, -2, length=100)\n",
    "\n",
    "ridge.mod <- glmnet(modelmat.train, train$Salary, alpha=0, lambda=grid) # alpha=0 for ridge, 1 for lasso\n",
    "\n",
    "## Choosing optimal lambda ##\n",
    "\n",
    "ridge.cv.out <- cv.glmnet(modelmat.train, train$Salary, alpha=0)\n",
    "plot(ridge.cv.out)\n",
    "bestlam <- ridge.cv.out$lambda.min\n",
    "bestlam # = 0.0634\n",
    "\n",
    "modelmat.test <- model.matrix(Salary~., data=test)[,-1]\n",
    "\n",
    "ridge.mod <- glmnet(modelmat.train, train$Salary, alpha=0)\n",
    "ridge.pred <- predict(ridge.mod, newx = modelmat.test, s=bestlam) #s= shrinkage, the best model lambda we found earlier\n",
    "mse.ridge <- mean((ridge.pred-test.y)^2)\n",
    "mse.ridge # = 0.4568\n",
    "\n",
    "## Lasso ##\n",
    "\n",
    "# Finding the best value for lambda ##\n",
    "\n",
    "lasso.cv.out <- cv.glmnet(modelmat.train, train$Salary, alpha=1)\n",
    "plot(lasso.cv.out)\n",
    "bestlam.lasso <- lasso.cv.out$lambda.min\n",
    "bestlam.lasso\n",
    "\n",
    "lasso.mod <- glmnet(modelmat.train, train$Salary, alpha=1, lambda=bestlam.lasso)\n",
    "lasso.pred.coefs <- predict(lasso.mod, type=\"coefficients\", newx = modelmat.test)\n",
    "lasso.pred.response <- predict(lasso.mod, newx = modelmat.test)\n",
    "mse.lasso <- mean((lasso.pred.response-test.y)^2)\n",
    "mse.lasso # = 0.469\n",
    "\n",
    "## Best subset selection ##\n",
    "\n",
    "library(leaps)\n",
    "\n",
    "regfit.full <- regsubsets(Salary~., data=train, nvmax=ncol(train)-1)\n",
    "summary.regfit.full <- summary(regfit.full)\n",
    "summary.regfit.full\n",
    "\n",
    "par(mfrow=c(3,1))\n",
    "\n",
    "plot(1:19, summary.regfit.full$bic, type=\"b\")\n",
    "min.bic <- which.min(summary.regfit.full$bic)\n",
    "points(min.bic, summary.regfit.full$bic[min.bic], col=\"red\", pch=16)\n",
    "\n",
    "plot(1:19, summary.regfit.full$cp, type=\"b\")\n",
    "min.cp <- which.min(summary.regfit.full$cp)\n",
    "points(min.cp, summary.regfit.full$cp[min.cp], col=\"red\", pch=16)\n",
    "\n",
    "plot(1:19, summary.regfit.full$adjr2, type=\"b\")\n",
    "max.adjr2 <- which.max(summary.regfit.full$adjr2)\n",
    "points(max.adjr2, summary.regfit.full$adjr2[max.adjr2], col=\"red\", pch=16)\n",
    "\n",
    "# choosing 7 variables as a happy medium\n",
    "\n",
    "regfit.best <- regsubsets(Salary~., data=train, nvmax=7)\n",
    "\n",
    "predict.regsubsets =function (object , newdata ,id ,...){\n",
    "form=as.formula (object$call [[2]])\n",
    "mat=model.matrix(form ,newdata )\n",
    "coefi=coef(object ,id=id)\n",
    "xvars=names(coefi)\n",
    "mat[,xvars]%*%coefi\n",
    "}\n",
    "\n",
    "bss.pred <- predict.regsubsets(regfit.full, test, 7)\n",
    "mse.bss <- mean((bss.pred-y.test)^2)\n",
    "mse.bss # = 0.479\n",
    "\n",
    "# Plotting the MSEs #\n",
    "\n",
    "mse.methods <- c(mse.boost, mse.lm, mse.ridge, mse.lasso, mse.bss)\n",
    "\n",
    "barplot(mse.methods, names=c(\"boost\",\"linear\",\"ridge\",\"lasso\",\"bss\"), xlab=\"Method\", ylab=\"Test MSE\")\n",
    "\n",
    "#f)\n",
    "\n",
    "\n",
    "\n",
    "summary(boost.hitters)\n",
    "\n",
    "# CAtBat is the most important factor, followed by CRBI, CRuns, CHits and CWalks\n",
    "\n",
    "#g) # Bagging on the training set\n",
    "\n",
    "library(randomForest)\n",
    "bag.hitters <- randomForest(Salary~., data=train, mtry=ncol(train)-1)\n",
    "pred.bag <- predict(bag.hitters, newdata = test)\n",
    "bag.mse <- mean((pred.bag-y.test)^2)\n",
    "bag.mse # = 0.233\n",
    "\n",
    "#11)\n",
    "#a)\n",
    "data(\"Caravan\")\n",
    "\n",
    "dim(Caravan)\n",
    "\n",
    "trainid <- 1:1000\n",
    "\n",
    "train <- Caravan[trainid,]\n",
    "test <- Caravan[-trainid,]\n",
    "\n",
    "#b)\n",
    "\n",
    "Caravan$Purchase <- ifelse(Caravan$Purchase==\"Yes\",1,0)\n",
    "table(Caravan$Purchase)\n",
    "\n",
    "train <- Caravan[trainid,]\n",
    "test <- Caravan[-trainid,]\n",
    "\n",
    "caravan.boost <- gbm(Purchase~., data=train, distribution = \"bernoulli\", n.trees = 1000, shrinkage=0.01) ## Would usually cross-validate for the shrinkage value, but here it is provided\n",
    "\n",
    "summary(caravan.boost)\n",
    "\n",
    "#PPERSAUT is the most important, followed by MKOOPKLA and MOPLHOOG\n",
    "\n",
    "#c)\n",
    "\n",
    "boost.preds <- predict(caravan.boost, newdata = test, n.trees = 1000, type=\"response\")\n",
    "\n",
    "head(boost.preds)\n",
    "\n",
    "boost.preds <- ifelse(boost.preds > 0.2, 1, 0)\n",
    "table(boost.preds, test$Purchase)\n",
    "\n",
    "# The model predicts that 144 people will make a purchase, and 31 actually do for a correct prediction rate of 21.5%\n",
    "\n",
    "## KNN ##\n",
    "\n",
    "# Cross validation for best k-value\n",
    "\n",
    "knn.errs <- rep(NA,20)\n",
    "\n",
    "x.train <- train[,-train$Purchase]\n",
    "x.test <- test[,-test$Purchase]\n",
    "y.train <- train$Purchase\n",
    "y.test <- test$Purchase\n",
    "\n",
    "for(i in 1:20) {\n",
    "\n",
    "caravan.knn <- knn(x.train, x.test, y.train, k=i)\n",
    "knn.errs[i] <- sum(caravan.knn!=y.test)\n",
    "\n",
    "}\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "plot(knn.errs, type=\"b\")\n",
    "\n",
    "# k=11 seems to show the smallest error\n",
    "\n",
    "caravan.knn <- knn(x.train, x.test, y.train, k=11)\n",
    "\n",
    "head(caravan.knn) # KNN predict thats nobody would make a purchase\n",
    "\n",
    "## Logistic Regression ##\n",
    "\n",
    "logistic.mod <- glm(Purchase~., data=train, family=binomial)\n",
    "logistic.pred <- predict(logistic.mod, family=\"binomial\", newdata = test, type=\"response\")\n",
    "table(logistic.pred)\n",
    "\n",
    "logistic.pred <- ifelse(logistic.pred > 0.2, 1, 0)\n",
    "\n",
    "table(logistic.pred, y.test)\n",
    "# Logistic regression predicts that 408 would buy and 58 did so correct = 14.2%\n",
    "\n",
    "#12)\n",
    "\n",
    "data(Weekly)\n",
    "\n",
    "## Predicting Direction based on all other variables using bagging, boosting, random forest, logistic regression ##\n",
    "\n",
    "dim(Weekly)\n",
    "\n",
    "set.seed(1)\n",
    "trainid <- sample(1:nrow(Weekly), nrow(Weekly)/2, replace=F)\n",
    "\n",
    "Weekly$Direction <- ifelse(Weekly$Direction ==\"Up\", 1, 0)\n",
    "Weekly <- Weekly[,-c(1,8)]\n",
    "\n",
    "train <- Weekly[trainid,]\n",
    "test <- Weekly[-trainid,]\n",
    "\n",
    "# Logistic Regression #\n",
    "\n",
    "y.test <- test$Direction\n",
    "\n",
    "\n",
    "dir.glm <- glm(Direction~., data=train, family=\"binomial\") # Take away Today because it 100% predicts Direction, returns an error\n",
    "glm.probs <- predict(dir.glm, newdata = test, type = \"response\")\n",
    "glm.pred <- ifelse(glm.probs >=0.5,1,0)\n",
    "\n",
    "table(y.test, glm.pred) # Classification error rate = 233/545 = 42.75%. We would expect the null model to have a classification error rate of 50%.\n",
    "\n",
    "logit.accuracy <- 0.5376\n",
    "\n",
    "# Boosting #\n",
    "\n",
    "# Cross validate for best lambda value\n",
    "\n",
    "powers <- seq(-3,-0.1,0.1)\n",
    "lambdas <- 10^powers\n",
    "errors <- rep(NA,length(powers))\n",
    "\n",
    "\n",
    "for(i in seq_along(lambdas)) {\n",
    "\n",
    "  dir.boost <- gbm(Direction~., data=train, n.trees=1000, distribution = \"bernoulli\", shrinkage=lambdas[i])\n",
    "\n",
    "  pred <- predict(dir.boost, newdata = test, n.trees = 1000, type=\"response\")\n",
    "  pred.binary <- ifelse(pred >= 0.5, 1, 0)\n",
    "  \n",
    "  errors[i] <- sum(pred.binary!=y.test)\n",
    "}\n",
    "\n",
    "plot(errors, type=\"b\") \n",
    "\n",
    "points(which.min(errors), errors[which.min(errors)], pch=16, col=\"red\")\n",
    "\n",
    "best.lambda <- lambdas[which.min(errors)]\n",
    "best.lambda # = 0.1585\n",
    "\n",
    "# Cross validating for best value for interaction depth\n",
    "\n",
    "cv.errors <- rep(NA, 10)\n",
    "\n",
    "for (i in 1:10) {\n",
    "  \n",
    "  dir.boost <- gbm(Direction~., data=train, n.trees=1000, distribution = \"bernoulli\", shrinkage=best.lambda, interaction.depth = i)\n",
    "\n",
    "  pred <- predict(dir.boost, newdata = test, n.trees = 1000, type=\"response\")\n",
    "  pred.binary <- ifelse(pred >= 0.5, 1, 0)\n",
    "  \n",
    "  cv.errors[i] <- sum(pred.binary!=y.test)\n",
    "}\n",
    "\n",
    "plot(cv.errors, type=\"b\") \n",
    "points(6, cv.errors[6], pch=16, col=\"red\")\n",
    "best.interaction.depth <- which.min(cv.errors)\n",
    "best.interaction.depth\n",
    "\n",
    "# Fitting the full model with the test set\n",
    "\n",
    "dir.boost <- gbm(Direction~., data=train, n.trees=1000, distribution = \"bernoulli\", shrinkage=best.lambda, interaction.depth = best.interaction.depth)\n",
    "boost.probs <- predict(dir.boost, newdata = test, n.trees = 1000, type=\"response\")\n",
    "boost.preds <- ifelse(boost.probs >= 0.5, 1, 0)\n",
    "\n",
    "table(boost.preds, y.test) # Prediction accuracy of 299/545 = 54.86%\n",
    "\n",
    "boost.accuracy <- 0.5486\n",
    "\n",
    "# Bagging #\n",
    "\n",
    "dir.bag <- randomForest(Direction~., data=train, mtry=ncol(train)-1)\n",
    "probs.bag <- predict(dir.bag, newdata = test, type=\"response\")\n",
    "preds.bag <- ifelse(probs.bag >= 0.5, 1, 0)\n",
    "table(preds.bag, y.test) # Prediction accuracy of 302/545 = 55.41%\n",
    "\n",
    "bag.accuracy <- 0.5541\n",
    "\n",
    "# Random Forest #\n",
    "\n",
    "dir.rf <- randomForest(Direction~., data=train, mtry=(ncol(train)-1)/2)\n",
    "dir.rf2 <- randomForest(Direction~., data=train, mtry=(ncol(train)-1)/3)\n",
    "dir.rf3 <- randomForest(Direction~., data=train, mtry=sqrt(ncol(train)))\n",
    "dir.rf4 <- randomForest(Direction~., data=train, mtry=2, ntree=500)\n",
    "\n",
    "probs.rf <- predict(dir.rf, newdata = test, type = \"response\")\n",
    "probs.rf2 <- predict(dir.rf2, newdata = test, type = \"response\")\n",
    "probs.rf3 <- predict(dir.rf3, newdata = test, type = \"response\")\n",
    "probs.rf3 <- predict(dir.rf3, newdata = test, type = \"response\")\n",
    "\n",
    "preds.rf <- ifelse(probs.rf >= 0.5, 1, 0)\n",
    "preds.rf2 <- ifelse(probs.rf2 >= 0.5, 1, 0)\n",
    "preds.rf3 <- ifelse(probs.rf3 >= 0.5, 1, 0)\n",
    "\n",
    "table(preds.rf, y.test) # 300/545\n",
    "table(preds.rf2, y.test) # 298/545\n",
    "table(preds.rf3, y.test) # 290/545\n",
    "\n",
    "# mtry=(ncol(train)-1)/2 has the best predictive results\n",
    "\n",
    "rf.accuracy <- 0.5504\n",
    "\n",
    "results <- c(0.5, logit.accuracy, boost.accuracy, bag.accuracy, rf.accuracy)\n",
    "\n",
    "barplot(results, main=\"Prediction Accuracy\", names=c(\"Chance\",\"Logit\",\"Boosting\",\"Bagging\",\"Random Forest\"), ylim=c(0.4,0.6))\n",
    "\n",
    "# Bagging shows the greatest predictive capability \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
