{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Chapter 4 - Classificatoin\"\n",
    "author: \"Dan\"\n",
    "date: \"4 February 2018\"\n",
    "output: html_document\n",
    "editor_options: \n",
    "  chunk_output_type: console\n",
    "---\n",
    "\n",
    "```{r setup, include=FALSE}\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "```\n",
    "```{r}\n",
    "data(Smarket)\n",
    "names(Smarket)\n",
    "dim(Smarket)\n",
    "summary(Smarket)\n",
    "```\n",
    "\n",
    "```{r}\n",
    "cor.prob <- function (X, dfr = nrow(X) - 2) {\n",
    "  R <- cor(X, use=\"pairwise.complete.obs\")\n",
    "  above <- row(R) < col(R)\n",
    "  r2 <- R[above]^2\n",
    "  Fstat <- r2 * dfr/(1 - r2)\n",
    "  R[above] <- 1 - pf(Fstat, 1, dfr)\n",
    "  R[row(R) == col(R)] <- NA\n",
    "  R\n",
    "}\n",
    "\n",
    "flattenSquareMatrix <- function(m) {\n",
    "  if( (class(m) != \"matrix\") | (nrow(m) != ncol(m))) stop(\"Must be a square matrix.\") \n",
    "  if(!identical(rownames(m), colnames(m))) stop(\"Row and column names must be equal.\")\n",
    "  ut <- upper.tri(m)\n",
    "  data.frame(i = rownames(m)[row(m)[ut]],\n",
    "             j = rownames(m)[col(m)[ut]],\n",
    "             cor=t(m)[ut],\n",
    "             p=m[ut])\n",
    "}\n",
    "\n",
    "mydata <- Smarket[-9]\n",
    "corr <- flattenSquareMatrix(cor.prob(mydata))\n",
    "\n",
    "library(dplyr)\n",
    "\n",
    "corr %>%\n",
    "  arrange(p)\n",
    "\n",
    "## Only one with a p-value < 0.05 is year and volume, as we would expect there is no correlation ##\n",
    "\n",
    "library(ggplot2)\n",
    "\n",
    "ggplot(data = mydata, aes(x=Year, y=Volume, color = Year)) + geom_point() + geom_smooth(method=\"lm\", se=0) + geom_jitter(height=0, width=0.05)\n",
    "\n",
    "\n",
    "```\n",
    "```{r}\n",
    "## Lets see if we can predict the direction of movement given the change in the previous 5 days\n",
    "\n",
    "glm_fit <- glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)\n",
    "\n",
    "summary(glm_fit)\n",
    "\n",
    "## Doesn't look like any of these predictors have any significance.\n",
    "\n",
    "coef(glm.fit)\n",
    "glm_probs <- predict(glm_fit, type=\"response\")\n",
    "attach(Smarket)\n",
    "contrasts(Direction)\n",
    "\n",
    "## In order to make a prediction, we need to turn these probabilities into the class labels \"Up\" and \"Down\"\n",
    "\n",
    "library(broom)\n",
    "glm_probs <- augment(glm_fit, type.predict = \"response\")\n",
    "\n",
    "library(dplyr)\n",
    "glm_probs %>%\n",
    "  mutate(direction_hat = round(.fitted)) %>%\n",
    "  select (Direction, direction_hat) %>%\n",
    "  table() ## Creates a confusion matrix\n",
    "\n",
    "145+507 # sum of the diagonals are the correct predictions\n",
    "correct_predictions_pct <- 652/1250\n",
    "correct_predictions_pct\n",
    "\n",
    "## At first glance, is appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of 1250 observations. \n",
    "\n",
    "## 100-52.2 = 47.8% training error rate. As we have seen previously, the training error rate is often overly optimistic - it tends to under-estimate the the test error rate.\n",
    "\n",
    "## To better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, then examine how well it predicts on the held out data. \n",
    "\n",
    "train <- (Year<2005) # 1250 element boolean vector\n",
    "Smarket.2005 <- Smarket[!train,] # 252 x 9 matrix \n",
    "dim(Smarket.2005)\n",
    "Direction.2005 <- Direction[!train] # \n",
    "\n",
    "glm.fits <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data = Smarket, family = binomial, subset = train) # fit the model with the training set\n",
    "\n",
    "glm.probs <- predict(glm.fits, type = \"response\", newdata = Smarket.2005) # add new data to see how well we predict\n",
    "\n",
    "\n",
    "glm.pred <- rep(\"Down\",252) # 252 element vector of \"Down\"\n",
    "glm.pred[glm.probs>0.5] = \"Up\" # Change the indexes where glm.probs >0.5 to \"Up\"\n",
    "table(glm.pred,Direction.2005) # tabulate the two vectors of equal length\n",
    "\n",
    "# (77+44)/252 = 0.48 # Correct predictions\n",
    "# 1-0.48 = 0.52 = test error rate, worse than chance\n",
    "\n",
    "# We recall that the logistic regression model had very underwhelming pvalues\n",
    "# associated with all of the predictors, and that the smallest p-value,\n",
    "# though not very small, corresponded to Lag1. Perhaps by removing the\n",
    "# variables that appear not to be helpful in predicting Direction, we can\n",
    "# obtain a more effective model. After all, using predictors that have no\n",
    "# relationship with the response tends to cause a deterioration in the test\n",
    "# error rate (since such predictors cause an increase in variance without a\n",
    "# corresponding decrease in bias), and so removing such predictors may in\n",
    "# turn yield an improvement. Below we have refit the logistic regression using\n",
    "# just Lag1 and Lag2, which seemed to have the highest predictive power in\n",
    "# the original logistic regression model.\n",
    "\n",
    "glm.fits <- glm(Direction~Lag1+Lag2, data=Smarket, family = binomial, subset = train)\n",
    "\n",
    "glm.probs <- predict(glm.fits, Smarket.2005, type = \"response\")\n",
    "\n",
    "glm.pred <- rep(\"Down\", 252)\n",
    "glm.pred[glm.probs > 0.5] <- \"Up\"\n",
    "table(glm.pred,Direction.2005)\n",
    "\n",
    "mean(glm.pred!=Direction.2005) # Test error rate\n",
    "  # (35+106)/252 = correct 56% of the time\n",
    " # Remember that if we just guessed increase every day we will also be correct 56% of the time! Hence, in terms of the overall error rate, the logistic regression model is no better than the naive approach. \n",
    "\n",
    "# 106/(106/76) = how often the models predictions that the market would increase were accurate = 58%\n",
    "\n",
    "# This suggests a possible trading strategy of buying on days where the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course, one would need to investigate more carefully whether this small improvement was real or just due to random chance.\n",
    "\n",
    "# Suppose that we want to predict the resurns associated with particular values of Lag1 and Lag2. In particular, we want to predict Direction on a day when Lag1 and Lag2 equal 1.1 and 1.2 respectively, and on a day when they equal 1.5 and -0.8. We can do this with the predict() function\n",
    "\n",
    "predict(glm.fits, newdata=data.frame(Lag1=c(1.2, 1.5), Lag2=c(1.1, -0.8)), type=\"response\") # The model predicts that for these numbers of Lag1 and Lag2, the market direction will be down in both cases.\n",
    "\n",
    "## Linear Discriminant Analysis ##\n",
    "\n",
    "# lda() function is part of the MASS package\n",
    "\n",
    "library(MASS)\n",
    "lda.fit <- lda(Direction~Lag1+Lag2, data=Smarket, subset=train)\n",
    "\n",
    "# Prior probabilities = pi1 and pi2 values = 0.491 and 0.508\n",
    "# Group means are the mu values, the mean value of each predictor within each class. Shows that on days where the market goes Down, the previous two days tended to be positive, and when the market goes Up, the previous two days tended to be negative\n",
    "\n",
    "# The coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements X=x in (4.19)\n",
    "\n",
    "# If -0.642 x Lag1 - 0.514 x Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a decline.\n",
    "\n",
    "plot(lda.fit) # plots the linear discriminants, obtained by computing -0.642 x Lag1 - 0.514 x Lag2 for each of the training observations\n",
    "\n",
    "lda.pred <-predict(lda.fit, newdata = Smarket.2005) # Returns a list with three elements. The first, class, is a factor vector of Ups and Downs. The second is a 252x2 matrix whose kth column corresponds to the posterior probability of the corresponding training observation going into the kth class, computed from (4.10). Finally, x contains the linear discriminants, described above \n",
    "\n",
    "lda.class <- lda.pred$class\n",
    "table(lda.class, Direction.2005)\n",
    "\n",
    "mean(lda.class==Direction.2005) # = 56% correct, LDA and logistic regression predictions are almost identical\n",
    "\n",
    "# Applying a 50% threshold to the posterior probabilities allows us to re-create the predictions contained in lda.pred$class\n",
    "\n",
    "sum(lda.pred$posterior[,1]>=.5) # number of elements of coulumn 1 of the posterior probability matrix (Down) that are favourite = 70\n",
    "\n",
    "sum(lda.pred$posterior[,1] < 0.5) # 252 minus above\n",
    "\n",
    "#If we wanted to use a posterior probability threshold other than 50% in order the make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indded dectease on that day - say, if the posterior probability is at least 90%\n",
    "\n",
    "sum(lda.pred$posterior[,1]>0.9) # =0. No days met the threshold\n",
    "max(lda.pred$posterior[,1]) # max posterior probability was 52.02%\n",
    "\n",
    "## Quadratic Discriminant Analysis ##\n",
    "\n",
    "#Same syntax as LDA, also in the MASS package\n",
    "\n",
    "library(MASS)\n",
    "qda.fit <- qda(Direction~Lag1+Lag2, data=Smarket, subset=train)\n",
    "qda.class <- predict(qda.fit, newdata = Smarket.2005)$class\n",
    "table(qda.class, Direction.2005)\n",
    "mean(qda.class == Direction.2005) # 59.92% test success rate!\n",
    "\n",
    "# Suggests that the quadratic form assumed by QDA might capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this methods performance on a larger set before betting that this approach will consistently beat the market!\n",
    "\n",
    "## K-Nearest Neighbours ##\n",
    "\n",
    "# knn () function from the 'class' library\n",
    "\n",
    "# This function works differently from the other model fitting functions. Rather than a two step approach of fitting the model then using the model to make predictions, knn() forms predictions using a single command. The function requires 4 inputs\n",
    "\n",
    "# 1. A matrix containing the predictors associated with the training data, laballed Train.X \n",
    "\n",
    "# 2. A matrix containing the predictors associated with the data for which we wish to make predictions\n",
    "\n",
    "# 3. A vector containing the class labels for the training observations, labelled train.Direction below\n",
    "\n",
    "# 4. A value for K, the number of nearest neighbours to be used by the classifier\n",
    "\n",
    "library(class)\n",
    "\n",
    "train.X <- cbind(Lag1,Lag2)[train,] # cbnid Lag1 and Lag2 for the row indexes where the boolean vector 'train' = TRUE\n",
    "\n",
    "test.X <- cbind(Lag1,Lag2)[!train,] # cbnid Lag1 and Lag2 for the row indexes where the boolean vector 'train' = FALSE\n",
    "\n",
    "train.Direction <- Direction[train] # vector containing class labels for the training observations\n",
    "\n",
    "## KNN uses random numbers to break ties of two points are exactly the same distance away, so set.seed(1) to make results replicable\n",
    "\n",
    "set.seed(1)\n",
    "knn.pred <- knn(train.X, test.X, train.Direction, k=1) # returns a vector of predictions, hence uninterpretable for inference\n",
    "table(knn.pred, Direction.2005)\n",
    "\n",
    "mean(knn.pred != Direction.2005) # Test error rate of 50%\n",
    "\n",
    "knn.pred2 <- knn(train.X, test.X, train.Direction, k=3)\n",
    "table(knn.pred2, Direction.2005)\n",
    "mean(knn.pred2 != Direction.2005) # Test error rate of 46.42%\n",
    "\n",
    "# No improvement on further increasing K. It appears that for this data, QDA provides the best results of the methods that we have examined so far.\n",
    "\n",
    "## Applying KNN to the Caravan data set from the ISLR library\n",
    "\n",
    "library(ISLR)\n",
    "data(Caravan)\n",
    "attach(Caravan)\n",
    "dim(Caravan)\n",
    "?Caravan\n",
    "\n",
    "summary(Purchase)\n",
    "\n",
    "# Response variable is purches, only 348 of 5822 customers bought insurance for a rate of 6%\n",
    "\n",
    "# Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between observations, and hence on the KNN classifier, than variables that are on a small scale.\n",
    "\n",
    "# For instance, imagine a data set that contains two variables, Salary and Age (measured in dollars and years, respectively). As far as KNN is concerned, a differece of $1000 in salary is enormous compared to a difference of 50 years in age. Consequently, Salary will drive the KNN classification results, and age will have almost no effect.\n",
    "\n",
    "# This is contraty to our intuition that a salary differnce of $1000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese Yen, or if we measured Age in minutes, then we'd get quite different classification results from what we get if these two variables are measured in dollars and years.\n",
    "\n",
    "# A good way to handle this problem is to standardize the data so that all variables are given a mean of zero adn a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column 86 because it is qualitative.\n",
    "\n",
    "standardized.X <- scale(Caravan[,-86])\n",
    "var(Caravan[,1]) # variance of column 1 before stanardizing\n",
    "var(Caravan[,2]) # variance of column 2\n",
    "var(standardized.X[,1]) # after standardizing\n",
    "var(standardized.X[,2])\n",
    "\n",
    "#Create a test set of the first 1,000 observations, the rest will be the training set\n",
    "\n",
    "test <- 1:1000 # Numberic vector of 1 to 1,000\n",
    "train.X <- standardized.X[-test,] \n",
    "test.X <- standardized.X[test,]\n",
    "train.Y <- Purchase[-test]\n",
    "test.Y <- Purchase[test]\n",
    "set.seed(1)\n",
    "knn.pred <- knn(train.X, test.X, train.Y, k=1)\n",
    "\n",
    "mean(test.Y != knn.pred) # test error rate is 11.8% but we could get down to an error rate of just 6% by predicting \"No\" for every observation.\n",
    "\n",
    "# Suppose that there is some non-trivial cost to trying to sell insurance to a given individual. For instance, perhaps a salesperson must visit each potential customer. If the company tries to sell insurance to a random selection of people, then the success rate will be only 6%, which may be far too low given the costs involved. Instead, the company would like to try to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest. Instead, the fraction of individuals who buy insurance GIVEN THAT THE MODEL PREDICTS THAT THEY WILL is the number of interest.\n",
    "\n",
    "table(knn.pred, test.Y)\n",
    "\n",
    "# 9 people bought insurance out of the predicted 77 for a succes rate of 11.7% which is a big improvement on the 6% just approaching random people\n",
    "\n",
    "knn.pred2 <- knn(train.X, test.X, train.Y, k=3)\n",
    "table(knn.pred2,test.Y) # at K=3 we're up to 19.2%\n",
    "\n",
    "knn.pred3 <- knn(train.X, test.X, train.Y, k=5)\n",
    "table(knn.pred3,test.Y) # at K=5 we're up to 26.6%. This is over four times the rate of random guessing. It appears that KNN is finding some real patterns in a difficult data set!\n",
    "\n",
    "# As a comparison, lets fit a logistic regression model to the data set\n",
    "\n",
    "glm.fit <- glm(Purchase ~. , data=Caravan, family=binomial, subset=-test) # didn't have to put subset = Caravan[-test] because data is already defined\n",
    "glm.probs <- predict(glm.fit, newdata = Caravan[test,], type=\"response\")\n",
    "glm.pred <- rep(\"No\", 1000)\n",
    "glm.pred[glm.probs > 0.25] = \"Yes\" # All values over 0.25 probability are predicted yes to Purchase\n",
    "table(glm.pred,test.Y) # 11/33 = 33.3% success rate, over 5 times better than guessing!\n",
    "\n",
    "\n",
    "## Exercises Q5 ##\n",
    "\n",
    "# a) If the Bayes decision boundary is linear, then LDA will perform better on the test set because QDA will have increased variance without any corresponding decrease in bias. For the training set, QDA will perform better if it overfits\n",
    "\n",
    "# b) If the Bayes decision boundary is non-linear, then QDA will do better on the test set. LDA would have high bias. For the training set, QDA will do better also. \n",
    "\n",
    "# c) As sample size n increases, we expect the test prdiction of QDA relative to LDA to increase because QDA will suffer less from over-fitting when it has lots of observations. \n",
    "\n",
    "# d) False, QDA will over-fit the data so there will be higher variance with no improvement in bias when compared to LDA in this case. QDA might achieve a better error rate on the training set, but if the decision boundary is linear then QDA is not flexible in any predictive way.\n",
    "\n",
    "# 6a) Logistic Regression\n",
    "\n",
    "exp(-6+0.05*40+1*3.5)/(1+exp(-6+0.05*40+1*3.5)) # 0.38\n",
    "\n",
    "# 6b) log(P(X)/1-P(X)) = B0 + B1X1 + B2X2\n",
    "#     (log(0.5 / (1-0.5)) + 6 - 3.5)/0.05 = X2 = 50\n",
    "\n",
    "# 7) \n",
    "\n",
    "# mu1 = dividend YES group = 10\n",
    "# mu2 = dividend NO group = 0\n",
    "# pi1 = dividend YES prior probability = 0.8\n",
    "# pi2 = dividend NO group = 0.2\n",
    "# var is shared and = 36\n",
    "\n",
    "(0.8*exp(-1/(2*36)*(4-10)^2))/(0.8*exp(-1/(2*36)*(4-10)^2)+(1-0.8)*exp(-1/(2*36)*(4-0)^2)) # = 75.2%\n",
    "\n",
    "# 8) We do not have enough information to tell, we would need the exact test set error rate for KNN to make a decision\n",
    "\n",
    "# 9a) x/1-x = 0.37\n",
    "#     x = 0.37 - 0.37x # add 0.37x to both sides\n",
    "#     1.37x = 0.37\n",
    "#     x = 0.37/1.37 = 0.27 = 27%\n",
    "\n",
    "# b) odds = p(x)/1-p(x) = 0.16/1-0.16 = 0.19 \n",
    "\n",
    "## 10a)\n",
    "\n",
    "data(Weekly)\n",
    "?Weekly\n",
    "\n",
    "mydata <- Weekly[-9]\n",
    "cor(mydata)\n",
    "cormaster <- flattenSquareMatrix(cor.prob(mydata))\n",
    "cormaster %>%\n",
    "  arrange(p)\n",
    "\n",
    "attach(Weekly)\n",
    "plot(Year, Volume)\n",
    "plot(Lag2, Volume)\n",
    "pairs(Weekly)\n",
    "\n",
    "# Volume and Year seem to be closely correlated\n",
    "\n",
    "#b) \n",
    "\n",
    "glm.fit <- glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data = Weekly, family=binomial)\n",
    "\n",
    "summary(glm.fit)\n",
    "\n",
    "## Lag2 appears to be statistically significant\n",
    "\n",
    "#c)\n",
    "\n",
    "glm.probs <- predict(glm.fit, data=Weekly, type = \"response\")\n",
    "\n",
    "glm.preds <- rep(\"Down\", 1089)\n",
    "glm.preds[glm.probs > 0.5] <- \"Up\"\n",
    "table(glm.preds, Direction)\n",
    "\n",
    "# Correct predictions = (557+54)/1089 = 56.1%\n",
    "\n",
    "#d)\n",
    "\n",
    "test <- (Year>2008)\n",
    "dim(Weekly[!test,])\n",
    "\n",
    "training.set <- Weekly[!test,]\n",
    "test.set <- Weekly[test,]\n",
    "glm.train <- glm(Direction ~ Lag2, data=training.set, family=binomial)\n",
    "glm.probs <- predict(glm.train, newdata = test.set, type=\"response\")\n",
    "\n",
    "glm.preds <- rep(\"Down\", 104)\n",
    "glm.preds[glm.probs > 0.5] <- \"Up\"\n",
    "table(glm.preds, Direction[test])\n",
    "\n",
    "# Correct predictions = 65/104 = 62.5%\n",
    "\n",
    "#e)\n",
    "\n",
    "lda.train <- lda(Direction ~ Lag2, data=training.set)\n",
    "lda.probs <- predict(lda.train, newdata=test.set)$class # predict outputs a list of 3 elements, by putting $class on the end we just get a vector of predictions to throw straight into a table\n",
    "table(lda.probs, Direction[test])\n",
    "\n",
    "# Correct predictions = 65/104 = 62.5%\n",
    "\n",
    "#f)\n",
    "\n",
    "qda.fit <- qda(Direction ~ Lag2, data=training.set)\n",
    "qda.probs <- predict(qda.fit, newdata = test.set)$class\n",
    "table(qda.probs, Direction[test])\n",
    "\n",
    "\n",
    "\n",
    "# Correct predictions = 61/104 = 58.6%\n",
    "\n",
    "#g)\n",
    "\n",
    "set.seed(1)\n",
    "train.X <- as.matrix(Lag2[!test]) # matrix of the X values in the training set, if only one predictor we have to use as.matrix\n",
    "test.X <- as.matrix(Lag2[test])\n",
    "train.Y <- Direction[!test]\n",
    "\n",
    "knn.fit <- knn(train.X, test.X, train.Y, k=1)\n",
    "\n",
    "table(knn.fit, Direction[test])\n",
    "\n",
    "# Correct predictions = 52/104 =  50%\n",
    "\n",
    "#h) Logistic Regression and LDA give the same good results. \n",
    "\n",
    "#i)\n",
    "\n",
    "knn.fit2 <- knn(train.X, test.X, train.Y, k=3)\n",
    "table(knn.fit2, Direction[test])\n",
    "\n",
    "# Correct predictions = 58/104 = 55.8%\n",
    "\n",
    "knn.fit10 <- knn(train.X, test.X, train.Y, k=10)\n",
    "table(knn.fit10, Direction[test]) # Correct 59/104 = 56.7%\n",
    "\n",
    "knn.fit15 <- knn(train.X, test.X, train.Y, k=15)\n",
    "table(knn.fit15, Direction[test]) # Correct 61/104 = 58.7%\n",
    "\n",
    "knn.fit20 <- knn(train.X, test.X, train.Y, k=20)\n",
    "table(knn.fit20, Direction[test]) # Correct 61/104 = 58.7%\n",
    "\n",
    "attach(Weekly)\n",
    "\n",
    "test.set <- Year>2008\n",
    "\n",
    "training.set <- Weekly[!test.set,]\n",
    "test.set <- Weekly[test.set,]\n",
    "\n",
    "\n",
    "\n",
    "lda.fit2 <- lda(Direction ~ Lag2 + I(Lag2^2), data = training.set)\n",
    "lda.pred2 <- predict(lda.fit2, newdata = test.set)$class\n",
    "table(lda.pred2, test.set$Direction)\n",
    "\n",
    "# Adding exponential transformation = 64/104 = 61.5%\n",
    "\n",
    "#11)\n",
    "\n",
    "data(Auto)\n",
    "myAuto <- Auto\n",
    "dim(Auto)\n",
    "myAuto\n",
    "myAuto2 <-  mutate(myAuto, mpg01 = as.numeric(mpg > median(mpg)))\n",
    "  \n",
    "str(myAuto2)\n",
    "cormaster <- flattenSquareMatrix(cor.prob(myAuto2[-9]))\n",
    "\n",
    "cormaster %>%\n",
    "  filter(j==\"mpg01\")\n",
    "\n",
    "pairs(myAuto2[-9])\n",
    "\n",
    "# mpg01 is highly correlated with every variable apart from acceleration\n",
    "\n",
    "myAuto3 <- myAuto2[-9]\n",
    "\n",
    "#Splitting into training set and test set\n",
    "\n",
    "test <- seq(301:392)\n",
    "mydf2 <- mydf[-9]\n",
    "\n",
    "training.set <- mydf2[-test,]\n",
    "test.set <- mydf2[test,]\n",
    "\n",
    "lda.fit <- lda(mpg01 ~ displacement+horsepower+weight+acceleration, data = training.set)\n",
    "lda.preds <- predict(lda.fit, newdata = test.set)\n",
    "table(lda.preds$class, mydf2$mpg01[test])\n",
    "mean(lda.preds$class != mydf2$mpg01[test])\n",
    "\n",
    "# Test error rate = 14.1%\n",
    "\n",
    "qda.fit <- qda(mpg01 ~ displacement+horsepower+weight+acceleration, data = training.set)\n",
    "qda.preds <- predict(qda.fit, newdata = test.set) \n",
    "table(qda.preds$class, mydf2$mpg01[test])\n",
    "mean(qda.preds$class != mydf2$mpg01[test])\n",
    "\n",
    "# Test Error Rate = 8.7%\n",
    "\n",
    "glm.fit <- glm(mpg01 ~ displacement+horsepower+weight+acceleration, data = training.set, family=binomial)\n",
    "glm.probs <- predict(glm.fit, newdata = test.set, type = \"response\")\n",
    "glm.preds <- rep(\"0\", 92)\n",
    "glm.preds[glm.probs > 0.5] = \"1\"\n",
    "table(glm.preds, mydf2$mpg01[test])\n",
    "mean(glm.preds != mydf2$mpg01[test])\n",
    "\n",
    "# Test error rate = 13%\n",
    "\n",
    "standardized.X <- scale(mydf2[-9])\n",
    "\n",
    "train.X <- standardized.X[-test,]\n",
    "test.X <- as.matrix(standardized.X[test,])\n",
    "train.Y <- mydf2$mpg01[-test]\n",
    "test.Y <- mydf2$mpg01[test]\n",
    "\n",
    "knn.fit <- knn(train.X, test.X, train.Y, k=1)\n",
    "knn.fit <- knn(train.X, test.X, train.Y, k=5)\n",
    "knn.fit <- knn(train.X, test.X, train.Y, k=10)\n",
    "knn.fit <- knn(train.X, test.X, train.Y, k=6)\n",
    "\n",
    "mean(test.Y!=knn.fit)\n",
    "\n",
    "# K=1 Test Error rate of 9.8%\n",
    "# K=5 Test Error rate of 6.5%\n",
    "# K=10 Test Error rate of 6.5%\n",
    "# K=30 Test Error rate of 8.7%\n",
    "# K=6 Test Error rate of 5.4%\n",
    "\n",
    "####################################\n",
    "\n",
    "#11 his way\n",
    "\n",
    "mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)\n",
    "mydf <- data.frame(Auto, mpg01)\n",
    "\n",
    "pairs (mydf)\n",
    "\n",
    "#12)\n",
    "\n",
    "Power <- function(x) {\n",
    "  y <- 2^x\n",
    "  print(y)\n",
    "}\n",
    "\n",
    "Power(3)\n",
    "\n",
    "Power2 <- function(x, a) {\n",
    "  y <- x^a\n",
    "  print(y)\n",
    "}\n",
    "\n",
    "Power2(3,8)\n",
    "Power2(10,3)\n",
    "Power2(8,17)\n",
    "Power2(131,3)\n",
    "\n",
    "Power3 <- function(x, a) {\n",
    "  result <- x^a\n",
    "  return(plot(result))\n",
    "}\n",
    "\n",
    "Power3(1:10,2)\n",
    "\n",
    "#13\n",
    "\n",
    "data(Boston)\n",
    "crimmed <- ifelse(Boston$crim > median(Boston$crim), 1, 0)\n",
    "\n",
    "mydf <- data.frame(Boston, crimmed)\n",
    "\n",
    "str(mydf)\n",
    "\n",
    "pairs(mydf)\n",
    "\n",
    "#correlated with zn, indus, nox, rm, dis, ptratio, lstat\n",
    "\n",
    "train <- 1:406\n",
    "test <- 407:nrow(mydf)\n",
    "\n",
    "training.set <- mydf[train,]\n",
    "test.set <- mydf[-train,]\n",
    "\n",
    "glm.fit <- glm(crimmed ~ zn+indus+nox+rm+dis+ptratio+lstat, data=training.set, family = \"binomial\")\n",
    "glm.pred <- predict(glm.fit, newdata = test.set, type=\"response\")\n",
    "glm.probs <- rep(\"0\", 100)\n",
    "glm.probs[glm.pred >= 0.5] <- \"1\"\n",
    "table(glm.probs, mydf$crimmed[test])\n",
    "mean(glm.probs != mydf$crimmed[test])\n",
    "\n",
    "# Test error rate of 19%\n",
    "# Removing rm increases test error rate to 20%\n",
    "\n",
    "summary(glm.fit)\n",
    "\n",
    "# Lets remove lstat and zn\n",
    "\n",
    "glm.fit2 <- glm(crimmed ~ indus+nox+rm+dis+ptratio, data=training.set, family = \"binomial\")\n",
    "glm.pred2 <- predict(glm.fit, newdata = test.set, type=\"response\")\n",
    "glm.probs2 <- rep(\"0\", 100)\n",
    "glm.probs2[glm.pred2 >= 0.5] <- \"1\"\n",
    "table(glm.probs2, mydf$crimmed[test])\n",
    "mean(glm.probs2 != mydf$crimmed[test])\n",
    "summary(glm.fit2)\n",
    "\n",
    "# Remove dis\n",
    "\n",
    "glm.fit2 <- glm(crimmed ~ indus+nox+rm+ptratio, data=training.set, family = \"binomial\")\n",
    "glm.pred2 <- predict(glm.fit, newdata = test.set, type=\"response\")\n",
    "glm.probs2 <- rep(\"0\", 100)\n",
    "glm.probs2[glm.pred2 >= 0.5] <- \"1\"\n",
    "table(glm.probs2, mydf$crimmed[test])\n",
    "mean(glm.probs2 != mydf$crimmed[test])\n",
    "summary(glm.fit2)\n",
    "plot(glm.fit2)\n",
    "\n",
    "## LDA\n",
    "\n",
    "lda.fit <- lda(crimmed ~ indus+nox+rm+ptratio, data=training.set)\n",
    "lda.pred <- predict(lda.fit, newdata = test.set)\n",
    "table(lda.pred$class, mydf$crimmed[test])\n",
    "mean(lda.pred$class != mydf$crimmed[test])\n",
    "\n",
    "# Test error rate of 16%\n",
    "\n",
    "## QDA\n",
    "\n",
    "qda.fit <- qda(crimmed ~ indus+nox+rm+ptratio, data=training.set)\n",
    "qda.pred <- predict(qda.fit, newdata = test.set)\n",
    "table(qda.pred$class, mydf$crimmed[test])\n",
    "mean(qda.pred$class != mydf$crimmed[test])\n",
    "\n",
    "# Test error rate of 18%\n",
    "\n",
    "## KNN\n",
    "\n",
    "standardized.X <- scale(mydf[-15])\n",
    "\n",
    "train.X <- standardized.X[-test,]\n",
    "test.X <- standardized.X[test,]\n",
    "train.Y <- mydf$crimmed[-test]\n",
    "test.Y <- mydf$crimmed[test]\n",
    "\n",
    "knn.fit1 <- knn(train.X, test.X, train.Y, k=1)\n",
    "table(knn.fit1, test.Y)\n",
    "mean(knn.fit1 != test.Y)\n",
    "\n",
    "# Test error rate = 12%\n",
    "\n",
    "knn.fit5 <- knn(train.X, test.X, train.Y, k=5)\n",
    "table(knn.fit5, test.Y)\n",
    "mean(knn.fit5 != test.Y)\n",
    "\n",
    "# Test error rate = 9% is as low as we can do\n",
    "\n",
    "crim01 <- ifelse(Boston$crim > median(Boston$crim), 1, 0)\n",
    "mydf <- data.frame(Boston, crim01)\n",
    "pairs(mydf)\n",
    "sort(cor(mydf)[1,])\n",
    "\n",
    "cormaster <- flattenSquareMatrix(cor.prob(mydf))\n",
    "cormaster %>%\n",
    "  filter (i==\"crim\") %>%\n",
    "  arrange (p)\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
